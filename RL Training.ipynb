{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-14T09:58:33.552468Z",
     "start_time": "2024-01-14T09:58:33.482396Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-14T10:08:08.195602Z",
     "start_time": "2024-01-14T10:08:08.182498Z"
    }
   },
   "outputs": [],
   "source": [
    "from src import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-14T10:08:08.506898Z",
     "start_time": "2024-01-14T10:08:08.501765Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from collections import defaultdict\n",
    "from tqdm.auto import tqdm, trange\n",
    "import pickle as pkl\n",
    "from typing import Literal, get_type_hints\n",
    "\n",
    "import itertools as it\n",
    "import more_itertools as mit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-14T10:08:08.769596Z",
     "start_time": "2024-01-14T10:08:08.762320Z"
    }
   },
   "outputs": [],
   "source": [
    "def player_formatter(player: int) -> str:\n",
    "    symbol_mapping = {0: \" \", 1: \"X\", -1: \"O\"}\n",
    "    return symbol_mapping[player]\n",
    "    \n",
    "def state_formatter(state: tuple[int, ...]) -> str:\n",
    "    size = int(len(state) ** 0.5)\n",
    "    formatted_state = \"\\n\"\n",
    "    for i in range(size):\n",
    "        formatted_state += \"+---+---+---+\\n\"\n",
    "        row = state[i*size:(i+1)*size]\n",
    "        formatted_state += \"| \" + \" | \".join(player_formatter(cell)for cell in row) + \" |\\n\"\n",
    "    formatted_state += \"+---+---+---+\"\n",
    "    return formatted_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "+---+---+---+\n",
      "|   |   |   |\n",
      "+---+---+---+\n",
      "|   |   |   |\n",
      "+---+---+---+\n",
      "|   |   |   |\n",
      "+---+---+---+\n"
     ]
    }
   ],
   "source": [
    "ttt = TicTacToe(default_state_formatter=state_formatter)\n",
    "ttt.render()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-14T10:08:09.151796Z",
     "start_time": "2024-01-14T10:08:09.148090Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "q_table_policy = QTablePolicy.load('q_table', lr=0.1, name='Deterministic Q Table')\n",
    "q_table_policy_stochastic = QTablePolicy(q_table_policy.q_table, \n",
    "                                         lr=0.1,\n",
    "                                         stochastic=True,\n",
    "                                         temperature=0.5,\n",
    "                                         name='Stochastic Q Table')\n",
    "human_policy = PromptPolicy(player_formatter)\n",
    "min_max_policy = MinMaxPolicy(game_cls=TicTacToe)\n",
    "mcts_policy = MCTS(game_cls=TicTacToe)\n",
    "# q_table_policy = QTablePolicy(lr=0.1)\n",
    "# mlp_policy = MLPPolicy(Model(), lr=0.001)\n",
    "# mcts_policy = MCTS(ttt)\n",
    "# mlp_policy = MLPPolicy.load(Model(), 'mlp.pt', lr=0.001)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-14T10:08:09.625760Z",
     "start_time": "2024-01-14T10:08:09.574246Z"
    }
   }
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class MCTS in module src.policies.q_policies.mcts_policy:\n",
      "\n",
      "class MCTS(src.policies.q_policies.base_q_policy.BaseQPolicySingle)\n",
      " |  MCTS(game_cls, *args, **kwargs)\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      MCTS\n",
      " |      src.policies.q_policies.base_q_policy.BaseQPolicySingle\n",
      " |      src.policies.q_policies.base_q_policy.BaseQPolicy\n",
      " |      src.policies.base_policy.BasePolicy\n",
      " |      abc.ABC\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, game_cls, *args, **kwargs)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  back_propogate(self, node, reward)\n",
      " |  \n",
      " |  calculate_reward(self, node)\n",
      " |  \n",
      " |  execute_round(self)\n",
      " |      execute a selection-expansion-simulation-backpropagation round\n",
      " |  \n",
      " |  expand(self, node)\n",
      " |  \n",
      " |  find_node_matching_state(self, node, state)\n",
      " |  \n",
      " |  get_all_Qs(self, state: tuple[int, ...], player: int, action_space: set[int]) -> dict[int, float]\n",
      " |  \n",
      " |  get_best_move(self, node, exploration_constant)\n",
      " |  \n",
      " |  select_child(self, node)\n",
      " |  \n",
      " |  simulate_by_brute_force(self, node)\n",
      " |  \n",
      " |  simulate_by_sampling(self, node)\n",
      " |  \n",
      " |  simulated_game(self, node)\n",
      " |  \n",
      " |  update_Q(self, state: tuple[int, ...], player: int, action: int, Q: float) -> None\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from src.policies.q_policies.base_q_policy.BaseQPolicySingle:\n",
      " |  \n",
      " |  batch_get_all_Qs(self, states: list[tuple[int, ...]], players: list[int], action_spaces: list[set[int]]) -> list[dict[int, float]]\n",
      " |  \n",
      " |  batch_update_Q(self, states: list[tuple[int, ...]], players: list[int], actions: list[int], Qs: list[float]) -> float\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from src.policies.q_policies.base_q_policy.BaseQPolicy:\n",
      " |  \n",
      " |  __call__(self, state: tuple[int, ...], player: int, action_space: set[int]) -> int\n",
      " |      Call self as a function.\n",
      " |  \n",
      " |  batch_get_Q(self, states: list[tuple[int, ...]], players: list[int], actions: list[int]) -> list[float]\n",
      " |  \n",
      " |  get_Q(self, state: tuple[int, ...], player: int, action: int) -> float\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from src.policies.base_policy.BasePolicy:\n",
      " |  \n",
      " |  get_name(self) -> str\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from src.policies.base_policy.BasePolicy:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n"
     ]
    }
   ],
   "source": [
    "help(MCTS)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-14T10:08:10.012403Z",
     "start_time": "2024-01-14T10:08:10.003159Z"
    }
   },
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-14T10:08:10.628344Z",
     "start_time": "2024-01-14T10:08:10.599514Z"
    }
   },
   "outputs": [],
   "source": [
    "def play(ttt, policy=None, print_state=False):\n",
    "    if policy is None:\n",
    "        policy = RandomPolicy()\n",
    "    if not isinstance(policy, dict):\n",
    "        policy = {-1: policy, 1: policy}\n",
    "\n",
    "    tape = [dict(\n",
    "        player=None,\n",
    "        action_space=None,\n",
    "        action=None,\n",
    "        state=ttt.get_state(),\n",
    "        winner=0\n",
    "    )]\n",
    "\n",
    "    while (action_space := ttt.get_actions()) and not ttt.get_winner():\n",
    "        if print_state:\n",
    "            ttt.render()\n",
    "        player = ttt.player\n",
    "        action = ttt.agent_move(policy[player])\n",
    "        state = ttt.get_state()\n",
    "        winner = ttt.get_winner()\n",
    "        tape.append(dict(\n",
    "            player=player,\n",
    "            action_space=action_space,\n",
    "            action=action,\n",
    "            state=state,\n",
    "            winner=winner\n",
    "        ))\n",
    "    if print_state:\n",
    "        ttt.render()\n",
    "\n",
    "    tape.append(dict(\n",
    "        player=None,\n",
    "        action_space=set(),\n",
    "        action=None,\n",
    "        state=ttt.get_state(),\n",
    "        winner=ttt.get_winner()\n",
    "    ))\n",
    "    \n",
    "    return tape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-14T10:08:11.111585Z",
     "start_time": "2024-01-14T10:08:11.103885Z"
    }
   },
   "outputs": [],
   "source": [
    "def swap(state: np.ndarray, option: Literal[1, -1]):\n",
    "    return option * state\n",
    "\n",
    "def flip(state: np.ndarray, option: Literal[True, False]):\n",
    "    return np.fliplr(state) if option else state\n",
    "\n",
    "def rotate(state: np.ndarray, option: Literal[0, 1, 2, 3]):\n",
    "    return np.rot90(state, k=option)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-14T10:08:11.561330Z",
     "start_time": "2024-01-14T10:08:11.553769Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_param_options(fn, param='option'):\n",
    "    return get_type_hints(fn)['option'].__args__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-14T10:08:11.801886Z",
     "start_time": "2024-01-14T10:08:11.798804Z"
    }
   },
   "outputs": [],
   "source": [
    "def transform_state(raw_state, fns, opts):\n",
    "    state = raw_state.reshape([3, 3])\n",
    "    for fn, opt in zip(fns, opts):\n",
    "        state = fn(state, opt)\n",
    "    return state.flatten()\n",
    "\n",
    "def transform_actions(raw_actions, fns, opts):\n",
    "    actions = np.zeros(9)\n",
    "    actions[raw_actions] = 1\n",
    "    actions = transform_state(actions, fns, opts)\n",
    "    return np.nonzero(actions)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-14T10:08:12.439160Z",
     "start_time": "2024-01-14T10:08:12.430942Z"
    }
   },
   "outputs": [],
   "source": [
    "def bellman_equation(policy, reward, state, player, actions):\n",
    "    if not actions:\n",
    "        return reward\n",
    "    return reward + max(policy.get_Q(state, player, action) for action in actions)\n",
    "\n",
    "def replay_episode(tape, policy):\n",
    "    state_list, player_list, action_list, q_list = [], [], [], []\n",
    "    transformations = [swap, flip, rotate]\n",
    "\n",
    "    for pre, cur, nxt in mit.windowed(tape, 3):\n",
    "        raw_start_state = np.array(list(pre['state'])).astype(int)\n",
    "        raw_end_state = np.array(list(nxt['state'])).astype(int)\n",
    "        raw_action = np.array([cur['action']]).astype(int)\n",
    "        raw_action_space = np.array(list(nxt['action_space'])).astype(int)\n",
    "        player = cur['player']\n",
    "        reward = cur['player'] * nxt['winner']\n",
    "        \n",
    "        for opts in it.product(*map(get_param_options, transformations)):\n",
    "            start_state = tuple(transform_state(raw_start_state, transformations, opts).tolist())\n",
    "            end_state = tuple(transform_state(raw_end_state, transformations, opts).tolist())\n",
    "            action = transform_actions(raw_action, transformations, opts).item()\n",
    "            action_space = set(transform_actions(raw_action_space, transformations, opts).tolist())\n",
    "\n",
    "            new_q = bellman_equation(policy, reward, end_state, player, action_space)\n",
    "            state_list.append(start_state)\n",
    "            player_list.append(player)\n",
    "            action_list.append(action)\n",
    "            q_list.append(new_q)\n",
    "\n",
    "    return state_list, player_list, action_list, q_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-14T10:05:23.063313Z",
     "start_time": "2024-01-14T10:05:23.056994Z"
    }
   },
   "outputs": [],
   "source": [
    "def train(policy, episodes, epsilon=0.1):\n",
    "    eps_greedy_policy = EpsilonGreedyPolicy(policy, epsilon=epsilon)\n",
    "    for episode in trange(episodes):\n",
    "        ttt.reset(start_player=random.choice([1, -1]))\n",
    "        tape = play(ttt, policy=eps_greedy_policy)\n",
    "        state_list, player_list, action_list, q_list = replay_episode(tape, policy)\n",
    "        loss = policy.batch_update_Q(state_list, player_list, action_list, q_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-14T09:36:04.067627Z",
     "start_time": "2024-01-14T09:35:43.622Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/100000 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4ecf8e9c6a9641649503ce4af7f835a2"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[13], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43mq_table_policy_stochastic\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepisodes\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m100000\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[12], line 6\u001B[0m, in \u001B[0;36mtrain\u001B[0;34m(policy, episodes, epsilon)\u001B[0m\n\u001B[1;32m      4\u001B[0m ttt\u001B[38;5;241m.\u001B[39mreset(start_player\u001B[38;5;241m=\u001B[39mrandom\u001B[38;5;241m.\u001B[39mchoice([\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m]))\n\u001B[1;32m      5\u001B[0m tape \u001B[38;5;241m=\u001B[39m play(ttt, policy\u001B[38;5;241m=\u001B[39meps_greedy_policy)\n\u001B[0;32m----> 6\u001B[0m state_list, player_list, action_list, q_list \u001B[38;5;241m=\u001B[39m \u001B[43mreplay_episode\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtape\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpolicy\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      7\u001B[0m loss \u001B[38;5;241m=\u001B[39m policy\u001B[38;5;241m.\u001B[39mbatch_update_Q(state_list, player_list, action_list, q_list)\n",
      "Cell \u001B[0;32mIn[11], line 21\u001B[0m, in \u001B[0;36mreplay_episode\u001B[0;34m(tape, policy)\u001B[0m\n\u001B[1;32m     19\u001B[0m start_state \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mtuple\u001B[39m(transform_state(raw_start_state, transformations, opts)\u001B[38;5;241m.\u001B[39mtolist())\n\u001B[1;32m     20\u001B[0m end_state \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mtuple\u001B[39m(transform_state(raw_end_state, transformations, opts)\u001B[38;5;241m.\u001B[39mtolist())\n\u001B[0;32m---> 21\u001B[0m action \u001B[38;5;241m=\u001B[39m \u001B[43mtransform_actions\u001B[49m\u001B[43m(\u001B[49m\u001B[43mraw_action\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtransformations\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mopts\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mitem()\n\u001B[1;32m     22\u001B[0m action_space \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m(transform_actions(raw_action_space, transformations, opts)\u001B[38;5;241m.\u001B[39mtolist())\n\u001B[1;32m     24\u001B[0m new_q \u001B[38;5;241m=\u001B[39m bellman_equation(policy, reward, end_state, player, action_space)\n",
      "Cell \u001B[0;32mIn[10], line 10\u001B[0m, in \u001B[0;36mtransform_actions\u001B[0;34m(raw_actions, fns, opts)\u001B[0m\n\u001B[1;32m      8\u001B[0m actions \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mzeros(\u001B[38;5;241m9\u001B[39m)\n\u001B[1;32m      9\u001B[0m actions[raw_actions] \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m---> 10\u001B[0m actions \u001B[38;5;241m=\u001B[39m \u001B[43mtransform_state\u001B[49m\u001B[43m(\u001B[49m\u001B[43mactions\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfns\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mopts\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     11\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m np\u001B[38;5;241m.\u001B[39mnonzero(actions)[\u001B[38;5;241m0\u001B[39m]\n",
      "Cell \u001B[0;32mIn[10], line 5\u001B[0m, in \u001B[0;36mtransform_state\u001B[0;34m(raw_state, fns, opts)\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m fn, opt \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mzip\u001B[39m(fns, opts):\n\u001B[1;32m      4\u001B[0m     state \u001B[38;5;241m=\u001B[39m fn(state, opt)\n\u001B[0;32m----> 5\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mstate\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mflatten\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "train(q_table_policy_stochastic, episodes=100000)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Manual Play"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-14T09:45:16.754740Z",
     "start_time": "2024-01-14T09:45:16.652058Z"
    }
   },
   "outputs": [],
   "source": [
    "q_table_policy.save('q_table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "+---+---+---+\n",
      "|   |   |   |\n",
      "+---+---+---+\n",
      "|   |   |   |\n",
      "+---+---+---+\n",
      "|   |   |   |\n",
      "+---+---+---+\n",
      "\n",
      "+---+---+---+\n",
      "|   |   |   |\n",
      "+---+---+---+\n",
      "|   | X |   |\n",
      "+---+---+---+\n",
      "|   |   |   |\n",
      "+---+---+---+\n",
      "\n",
      "+---+---+---+\n",
      "| O |   |   |\n",
      "+---+---+---+\n",
      "|   | X |   |\n",
      "+---+---+---+\n",
      "|   |   |   |\n",
      "+---+---+---+\n",
      "\n",
      "+---+---+---+\n",
      "| O | X |   |\n",
      "+---+---+---+\n",
      "|   | X |   |\n",
      "+---+---+---+\n",
      "|   |   |   |\n",
      "+---+---+---+\n",
      "\n",
      "+---+---+---+\n",
      "| O | X | O |\n",
      "+---+---+---+\n",
      "|   | X |   |\n",
      "+---+---+---+\n",
      "|   |   |   |\n",
      "+---+---+---+\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: ''",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[25], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m ttt\u001B[38;5;241m.\u001B[39mreset()\n\u001B[0;32m----> 2\u001B[0m tape \u001B[38;5;241m=\u001B[39m \u001B[43mplay\u001B[49m\u001B[43m(\u001B[49m\u001B[43mttt\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpolicy\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m{\u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mhuman_policy\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mmin_max_policy\u001B[49m\u001B[43m}\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mprint_state\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[20], line 19\u001B[0m, in \u001B[0;36mplay\u001B[0;34m(ttt, policy, print_state)\u001B[0m\n\u001B[1;32m     17\u001B[0m     ttt\u001B[38;5;241m.\u001B[39mrender()\n\u001B[1;32m     18\u001B[0m player \u001B[38;5;241m=\u001B[39m ttt\u001B[38;5;241m.\u001B[39mplayer\n\u001B[0;32m---> 19\u001B[0m action \u001B[38;5;241m=\u001B[39m \u001B[43mttt\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43magent_move\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpolicy\u001B[49m\u001B[43m[\u001B[49m\u001B[43mplayer\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     20\u001B[0m state \u001B[38;5;241m=\u001B[39m ttt\u001B[38;5;241m.\u001B[39mget_state()\n\u001B[1;32m     21\u001B[0m winner \u001B[38;5;241m=\u001B[39m ttt\u001B[38;5;241m.\u001B[39mget_winner()\n",
      "File \u001B[0;32m~/Projects/gomoku/src/tictactoe.py:31\u001B[0m, in \u001B[0;36mTicTacToe.agent_move\u001B[0;34m(self, policy)\u001B[0m\n\u001B[1;32m     30\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21magent_move\u001B[39m(\u001B[38;5;28mself\u001B[39m, policy):\n\u001B[0;32m---> 31\u001B[0m     best_action \u001B[38;5;241m=\u001B[39m \u001B[43mpolicy\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_state\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mplayer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_actions\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     32\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmove(best_action)\n\u001B[1;32m     33\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m best_action\n",
      "File \u001B[0;32m~/Projects/gomoku/src/policies/prompt_policy.py:15\u001B[0m, in \u001B[0;36mPromptPolicy.__call__\u001B[0;34m(self, state, player, actions)\u001B[0m\n\u001B[1;32m     13\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, state: \u001B[38;5;28mtuple\u001B[39m[\u001B[38;5;28mint\u001B[39m, \u001B[38;5;241m.\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;241m.\u001B[39m], player: \u001B[38;5;28mint\u001B[39m, actions: \u001B[38;5;28mset\u001B[39m[\u001B[38;5;28mint\u001B[39m]) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mint\u001B[39m:\n\u001B[1;32m     14\u001B[0m     time\u001B[38;5;241m.\u001B[39msleep(\u001B[38;5;241m0.1\u001B[39m)  \u001B[38;5;66;03m# to prevent the prompt from being printed before the game state\u001B[39;00m\n\u001B[0;32m---> 15\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mint\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;124;43mf\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mEnter action for \u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mformatter\u001B[49m\u001B[43m(\u001B[49m\u001B[43mplayer\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[38;5;124;43m: \u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mValueError\u001B[0m: invalid literal for int() with base 10: ''"
     ]
    }
   ],
   "source": [
    "ttt.reset()\n",
    "tape = play(ttt, policy={1: human_policy, -1: min_max_policy}, print_state=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-14T10:08:23.402777Z",
     "start_time": "2024-01-14T10:08:17.237189Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid decimal literal (66848535.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;36m  Cell \u001B[0;32mIn[17], line 2\u001B[0;36m\u001B[0m\n\u001B[0;31m    1pd.DataFrame(tape)\u001B[0m\n\u001B[0m    ^\u001B[0m\n\u001B[0;31mSyntaxError\u001B[0m\u001B[0;31m:\u001B[0m invalid decimal literal\n"
     ]
    }
   ],
   "source": [
    "\n",
    "1pd.DataFrame(tape)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-10T01:22:03.329624Z",
     "start_time": "2024-01-10T01:22:03.325330Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Run Simulations"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-14T09:46:37.253372Z",
     "start_time": "2024-01-14T09:46:37.250887Z"
    }
   },
   "outputs": [],
   "source": [
    "def simulate(round, player_policy_map, start_player=None):\n",
    "    player_policy_map[0] = DummyPolicy(name='Tie')\n",
    "    win_count = {policy.get_name(): 0 for policy in player_policy_map.values()}\n",
    "    for episode in trange(round):\n",
    "        ttt.reset(start_player=start_player or random.choice([-1, 1]))\n",
    "        tape = play(ttt, policy=player_policy_map)\n",
    "        win_count[player_policy_map[ttt.get_winner()].get_name()] += 1\n",
    "    return win_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-14T09:46:38.080427Z",
     "start_time": "2024-01-14T09:46:38.077360Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/10 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2d099917ec394872b002901312d1b82d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/10 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e97c572f6a234964a0bb4117465c8f60"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/10 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c3c404ae961043259569d3c6de7b1a8a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/10 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "37a1058dd2d749b89cc22526b5922f6e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "   Random Policy  MinMaxPolicy  Tie      Player -1       Player 1  \\\n0              4             6    0   MinMaxPolicy  Random Policy   \n1              0            10    0   MinMaxPolicy  Random Policy   \n2              1             9    0  Random Policy   MinMaxPolicy   \n3              0            10    0  Random Policy   MinMaxPolicy   \n\n    Start Player  \n0  Random Policy  \n1   MinMaxPolicy  \n2   MinMaxPolicy  \n3  Random Policy  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Random Policy</th>\n      <th>MinMaxPolicy</th>\n      <th>Tie</th>\n      <th>Player -1</th>\n      <th>Player 1</th>\n      <th>Start Player</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>4</td>\n      <td>6</td>\n      <td>0</td>\n      <td>MinMaxPolicy</td>\n      <td>Random Policy</td>\n      <td>Random Policy</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>10</td>\n      <td>0</td>\n      <td>MinMaxPolicy</td>\n      <td>Random Policy</td>\n      <td>MinMaxPolicy</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>9</td>\n      <td>0</td>\n      <td>Random Policy</td>\n      <td>MinMaxPolicy</td>\n      <td>MinMaxPolicy</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>10</td>\n      <td>0</td>\n      <td>Random Policy</td>\n      <td>MinMaxPolicy</td>\n      <td>Random Policy</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy_1 = RandomPolicy(name='Random Policy')\n",
    "# policy_1 = q_table_policy\n",
    "policy_2 = min_max_policy\n",
    "\n",
    "result_list = []\n",
    "for p1_role, start_player in [[1, 1], [1, -1], [-1, 1], [-1, -1]]:\n",
    "    policy_map = {1*p1_role: policy_1, -1*p1_role: policy_2}\n",
    "    result = simulate(10, policy_map, start_player=start_player)\n",
    "    result['Player -1'] = policy_map[-1].get_name()\n",
    "    result['Player 1'] = policy_map[1].get_name()\n",
    "    result['Start Player'] = policy_map.get(start_player).get_name()\n",
    "    result_list.append(result)\n",
    "pd.DataFrame(result_list)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-14T10:03:49.752871Z",
     "start_time": "2024-01-14T09:58:56.234787Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "deepnote": {},
  "deepnote_execution_queue": [],
  "deepnote_notebook_id": "1afec097b95f478fbf952ec9418e290f",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
